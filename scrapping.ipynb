{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too many warnings, Beautiful Soup is not beautiful at all \n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text File with all the links\n",
    "with open(\"/home/jay/Thesis/links.txt\") as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_and_subtopic(list_links):\n",
    "    \"\"\"Returns the Topic and Subtopic by HTML Parsing\"\"\"\n",
    "    topic = []\n",
    "    subtopic = []\n",
    "    topic_str = \"\"\n",
    "    subtopic_str = \"\"\n",
    "    # Topic extraction\n",
    "    for index,link in enumerate(list_links):\n",
    "        try:\n",
    "            top = re.search(r\"(\\/summary\\/chapter\\/)(\\w*_?\\w*?_?\\w*?)(.html)\", link)\n",
    "            #print(\"Found Topic: {}\".format(m.group(1)))                 # For debugging\n",
    "            topic.append(top.group(2))\n",
    "            topic_str = \"\".join(topic)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # Subtopic extraction\n",
    "    for index,link in enumerate(list_links):\n",
    "       \n",
    "        try:\n",
    "            sub = re.search(r\"(.\\/..\\/..\\/..\\/print-pdf.html\\?pageTitle=)(\\w*)\\+?(\\w*)?\\+?(\\w*)?\\+?(\\w*)?\\+?(\\w*)?\\+?(\\w*)?\\+?(\\w*)?\\+?(\\w*)?\\+?(\\w*)?\\+?(\\w*)?\", link)\n",
    "            #print(\"Found Topic: {}\".format(m.group(1)))                 # For debugging\n",
    "            \n",
    "            # There can be many groups, so we will take all groups except for group at index 0\n",
    "            \n",
    "            # Find lenght of sub list, count the number of subgroups\n",
    "            subtopic.append(sub.groups())\n",
    "            subtopic_str = \" \".join(subtopic[0][1:])\n",
    "            subtopic_str = subtopic_str.strip()\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return topic_str,subtopic_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates links\n",
    "\n",
    "def link_creation(list_links):\n",
    "    \"\"\"Returns list of links to extract text from\"\"\"\n",
    "    link_extract = []    # List to store all the links created\n",
    "    temp_list = []       # Temporary list to store all the numbers we get from regular expressions\n",
    "    \n",
    "    # Regular expressions to extract numbers\n",
    "    for index, link in enumerate(list_links):\n",
    "        try:\n",
    "            number = re.match(r\"(.\\/..\\/..\\/..\\/legal-content\\/EN\\/AUTO\\/\\?uri=legissum)\\:(\\w+?\\d+_?\\d+?)\",link)\n",
    "            temp_list.append(number.group(2))\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Creating list of links to extract text from \n",
    "    for index, number in enumerate(temp_list):\n",
    "        link_extract.append(\"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=LEGISSUM:\"+number+\"&from=EN\")\n",
    "\n",
    "    return link_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction function\n",
    "store_list_success = []     # List to store all the links that were successfully extracted\\\n",
    "store_list_failure = []     # List to store all the links that were not successfully extracted\\\n",
    "\n",
    "def extraction(hyper_links):\n",
    "    \n",
    "    \"\"\"Saves text in its native form from webpages\n",
    "       Returns information regarding files \"\"\"\n",
    "    \n",
    "    \n",
    "    # Get topic and subtopic\n",
    "    topic, subtopic = topic_and_subtopic(links)\n",
    "        \n",
    "    # stripping white spaces in topic and subtopic if any;\n",
    "        \n",
    "    topic = topic.replace(\" \",\"\")\n",
    "    subtopic = subtopic.replace(\" \",\"\")\n",
    "        \n",
    "    # Saving the text in appropiate folder\n",
    "    homedir = os.environ['HOME']\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        os.stat(homedir+\"/\"+\"Data/\"+topic+\"/\"+subtopic)\n",
    "    except:\n",
    "        os.makedirs(homedir+\"/\"+\"Data/\"+topic+\"/\"+subtopic)\n",
    "        \n",
    "    log_file = open(homedir+\"/\"+\"Data/\"+topic+\"/\"+subtopic+\"/\"+'log'+'.txt',\"w\")  #logging all the outputs to log file\n",
    "    \n",
    "    sys.stdout = log_file # Telling system to write all the print statements to log file\n",
    "    \n",
    "    for index, link in enumerate(hyper_links):\n",
    "        print(\"URL for Extraction is: {}\".format(link))\n",
    "        try:\n",
    "            htp = urllib.request.urlopen(link)\n",
    "            print(\"Waiting for response from webpage:{}\".format(link))\n",
    "            \n",
    "            soup = BeautifulSoup(htp, from_encoding=resp.info().get_param('charset'))\n",
    "            \n",
    "            # kill all script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()    # rip it out\n",
    "\n",
    "            # get text\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # break into lines and remove leading and trailing space on each\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            # break multi-headlines into a line each\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            # drop blank lines\n",
    "            text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Get the first line of text, that is the title of the summary\n",
    "            title = text.split('\\n', 1)[0]\n",
    "    \n",
    "                    \n",
    "            print(\"Saving Text to the file: {}.txt\".format(homedir+\"/\"+\"Data/\"+topic+\"/\"+subtopic+\"/\"+title))\n",
    "        \n",
    "            with open(homedir+\"/\"+\"Data/\"+topic+\"/\"+subtopic+\"/\"+title+'.txt', \"w\") as text_file:\n",
    "                text_file.write(text)\n",
    "                \n",
    "            # As the file is written successfully, we will store this like to successfull list\n",
    "            \n",
    "            # Updating the 'store_list_success'\n",
    "            store_list_success.append(link)\n",
    "            \n",
    "            \n",
    "        except IOError:\n",
    "            print(\"Unable to open link: {}\".format(link))\n",
    "            \n",
    "            # As we got no response from the webiste we are going to store the link in 'store_list_failure'\n",
    "            \n",
    "            # Updating the 'store_list_failure'\n",
    "            store_list_failure.append(link)\n",
    "            \n",
    "            pass\n",
    "           \n",
    "            \n",
    "        \n",
    "        # Information for display\n",
    "        \n",
    "        #information = str(homedir+\"/\"+\"Data/\"+topic+\"/\"+subtopic+\"/\"+title+'.txt')    # Location of file saved\n",
    "        #print(information)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, links in enumerate(lines):\n",
    "    resp = urllib.request.urlopen(links)\n",
    "    soup = BeautifulSoup(resp, from_encoding=resp.info().get_param('charset'))\n",
    "    \n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "    \n",
    "    # Topic and subtopic \n",
    "    topic, subtopic = topic_and_subtopic(links)\n",
    "    \n",
    "    print(\"Topic is:{} and Subtopic is:{}\".format(topic, subtopic))\n",
    "    \n",
    "    # Creating links:\n",
    "    print(\"Creating links for extraction...\")\n",
    "    \n",
    "    extraction_links = link_creation(links)      # Gets the list of links to extract contents from\n",
    "        \n",
    "    print(\"Link creation completet for topic:{} and subtopic:{}\".format(topic, subtopic))\n",
    "    \n",
    "    # Extraction Process:\n",
    "    print(\"Strating the extraction process...\")\n",
    "    \n",
    "    #for index, links_2_extract in enumerate(extraction_links):\n",
    "    extraction(extraction_links)\n",
    "    \n",
    "    print(\"Extraction Process completed.\")\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the 'store_list_success' and 'store_list_failure' into text file\n",
    "\n",
    "with open(homedir+\"/\"+\"Data/\"+\"success.txt\",\"a\") as fileStore:\n",
    "    fileStore.write(\"\\n\".join(store_list_success))\n",
    "\n",
    "with open(homedir+\"/\"+\"Data/\"+\"failure.txt\",\"a\") as file_Store:\n",
    "    file_Store.write(\"\\n\".join(store_list_failure))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
